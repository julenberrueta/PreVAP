{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fb3e5f-717d-4100-a6d6-7019a80641ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import experimental feature first\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "# Re-run the rest of the code now that IterativeImputer is properly enabled\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, GenericUnivariateSelect, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, recall_score,\n",
    "    precision_score, f1_score, confusion_matrix,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import optuna\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pymysql.cursors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from connections import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769e3c9-3419-44b7-9b65-fe61b912e5cb",
   "metadata": {},
   "source": [
    "# ELIMINAR FEATURES REDUNDANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cc2186-0a8a-49a0-99a6-bb29485334d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1) Create 5 random probes ====\n",
    "def add_random_probes(X: pd.DataFrame, n_probes: int = 5, seed: int = 42) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X_aug = X.copy()\n",
    "    for i in range(n_probes):\n",
    "        X_aug[f\"rand_probe_{i+1}\"] = rng.normal(loc=0.0, scale=1.0, size=len(X_aug))\n",
    "    return X_aug\n",
    "\n",
    "# ==== 2) Iterative Imputer ====\n",
    "def minimal_preprocess(X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Solo imputación iterativa sobre todas las columnas (asumidas numéricas).\n",
    "    Devuelve un DataFrame numérico sin NaNs, listo para VT + mRMR.\n",
    "    \"\"\"\n",
    "    imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "    X_imputed = imputer.fit_transform(X, y)\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "\n",
    "# ==== 3) Variance Threshold ====\n",
    "def apply_variance_threshold(X_num: pd.DataFrame, threshold: float = 0.01) -> pd.DataFrame:\n",
    "    vt = VarianceThreshold(threshold=threshold)\n",
    "    Xt = vt.fit_transform(X_num.values)\n",
    "    kept_mask = vt.get_support()\n",
    "    kept_cols = X_num.columns[kept_mask]\n",
    "    return pd.DataFrame(Xt, columns=kept_cols, index=X_num.index)\n",
    "\n",
    "# ==== 4) mRMR ranking ====\n",
    "def run_mrmr_ranking(X_num: pd.DataFrame, y: pd.Series, K: int | None = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    mrmr_classif devuelve lista de nombres ordenados (mejor → peor).\n",
    "    Asegúrate de pasar DataFrame numérico sin NaNs.\n",
    "    \"\"\"\n",
    "    if K is None:\n",
    "        K = X_num.shape[1]\n",
    "    return mrmr_classif(X=X_num, y=y, K=K)\n",
    "\n",
    "# ==== 5) Filter agains probes ====\n",
    "def filter_below_random_probes(ranking: list[str], probe_prefix: str = \"rand_probe_\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Mantiene solo las variables con ranking mejor que la peor random probe.\n",
    "    Si no hay probes (o no están en el ranking), no filtra nada.\n",
    "    \"\"\"\n",
    "    probe_positions = [i for i, f in enumerate(ranking) if f.startswith(probe_prefix)]\n",
    "    if not probe_positions:\n",
    "        return ranking  # no probes presentes\n",
    "    worst_probe_pos = max(probe_positions)  # índice más alto = peor entre las probes\n",
    "    return [f for i, f in enumerate(ranking) if i < worst_probe_pos and not f.startswith(probe_prefix)]\n",
    "\n",
    "# ==== 6) Pipeline end-to-end ====\n",
    "def feature_reduction_workflow(X_raw: pd.DataFrame, y: pd.Series,\n",
    "                               n_probes: int = 5,\n",
    "                               vt_threshold: float = 0.01,\n",
    "                               seed: int = 42) -> dict:\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - X_final: DataFrame reducido tras VT + mRMR + filtro contra probes\n",
    "      - ranking_all: ranking mRMR completo (incluyendo probes)\n",
    "      - kept_features: lista final de columnas mantenidas (sin probes)\n",
    "      - removed_by_vt: variables reales eliminadas por Variance Threshold\n",
    "      - removed_by_mrmr: variables reales eliminadas por el filtro mRMR (tras VT)\n",
    "      - removed_probes_by_vt: probes eliminadas por VT\n",
    "      - removed_probes_by_mrmr: probes eliminadas por el filtro mRMR (tras VT)\n",
    "      - vt_kept_features: columnas que sobrevivieron a VT (incluye probes si sobrevivieron)\n",
    "    \"\"\"\n",
    "    # Helper\n",
    "    def is_probe(name: str) -> bool:\n",
    "        return str(name).startswith(\"rand_probe_\")\n",
    "\n",
    "    # 1) Add probes\n",
    "    X_aug = add_random_probes(X_raw, n_probes=n_probes, seed=seed)\n",
    "\n",
    "    # 2) Imputation\n",
    "    X_pp = minimal_preprocess(X_aug, y)\n",
    "    pp_cols = list(X_pp.columns)\n",
    "\n",
    "    # 3) Variance Threshold\n",
    "    X_vt = apply_variance_threshold(X_pp, threshold=vt_threshold)\n",
    "    vt_kept_features = list(X_vt.columns)\n",
    "\n",
    "    # --- Removed by VT ---\n",
    "    removed_by_vt_all = [c for c in pp_cols if c not in vt_kept_features]\n",
    "    removed_by_vt = [c for c in removed_by_vt_all if not is_probe(c)]\n",
    "    removed_probes_by_vt = [c for c in removed_by_vt_all if is_probe(c)]\n",
    "\n",
    "    # 4) mRMR\n",
    "    ranking_all = run_mrmr_ranking(X_vt, y, K=X_vt.shape[1])\n",
    "\n",
    "    # 5) Filter by random probes (we keep the features ranked better thant the worst probe)\n",
    "    kept_features_with_probes = filter_below_random_probes(ranking_all, probe_prefix=\"rand_probe_\")\n",
    "\n",
    "    # 6) Final features without probes\n",
    "    kept_features = [f for f in kept_features_with_probes if not is_probe(f)]\n",
    "    X_final = X_vt[kept_features].copy()\n",
    "\n",
    "    # --- Removed by mRMR---\n",
    "    removed_by_mrmr_all = [c for c in vt_kept_features if c not in kept_features]\n",
    "    removed_by_mrmr = [c for c in removed_by_mrmr_all if not is_probe(c)]\n",
    "    removed_probes_by_mrmr = [c for c in removed_by_mrmr_all if is_probe(c)]\n",
    "\n",
    "    return {\n",
    "        \"X_final\": X_final,\n",
    "        \"ranking_all\": ranking_all,\n",
    "        \"kept_features\": kept_features,\n",
    "        \"removed_by_vt\": removed_by_vt,\n",
    "        \"removed_by_mrmr\": removed_by_mrmr,\n",
    "        \"removed_probes_by_vt\": removed_probes_by_vt,\n",
    "        \"removed_probes_by_mrmr\": removed_probes_by_mrmr,\n",
    "        \"vt_kept_features\": vt_kept_features,\n",
    "    }\n",
    "\n",
    "# ==== 7) Run the pipeline for -24h, -48h and -72h windows ====\n",
    "def run_by_window(df: pd.DataFrame,\n",
    "                  n_probes: int = 5, vt_threshold: float = 0.01) -> dict:\n",
    "\n",
    "    df_hr = df[df[\"hr\"].isin([-24, -48, -72])].drop(columns=[\"PatientID\", \"hr\"], errors=\"ignore\")\n",
    "        \n",
    "    y = df_hr[\"NAV\"].astype(int)\n",
    "    X = df_hr.drop(columns=[\"NAV\"], errors=\"ignore\")\n",
    "    return feature_reduction_workflow(X, y, n_probes=n_probes, vt_threshold=vt_threshold)\n",
    "\n",
    "def downsampling(X_train, y_train, majority_proportion=1.0):\n",
    "    \"\"\"\n",
    "    Performs downsampling on the majority class to balance the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (array-like): Feature matrix of the training data.\n",
    "    y_train (array-like): Label array corresponding to the training data.\n",
    "    majority_proportion (float or None): \n",
    "        - If float, ratio of majority class to minority class (e.g. 1.0 means balanced).\n",
    "        - If None, no downsampling is performed.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Downsampled (or original) feature matrix and label array as numpy arrays.\n",
    "    \"\"\"\n",
    "    X_train = np.asarray(X_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "\n",
    "    if majority_proportion is None:\n",
    "        return X_train, y_train\n",
    "\n",
    "    # Separate minority and majority class samples\n",
    "    X_minority = X_train[y_train == 1]\n",
    "    X_majority = X_train[y_train != 1]\n",
    "    y_majority = y_train[y_train != 1]\n",
    "\n",
    "    # Calculate number of majority class samples to keep\n",
    "    desired_majority_samples = int(len(X_minority) * majority_proportion)\n",
    "\n",
    "    # Downsample majority class\n",
    "    X_majority_downsampled, y_majority_downsampled = resample(\n",
    "        X_majority, y_majority,\n",
    "        replace=False,\n",
    "        n_samples=desired_majority_samples,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Combine minority and downsampled majority\n",
    "    X_balanced = np.vstack((X_minority, X_majority_downsampled))\n",
    "    y_balanced = np.hstack((np.ones(len(X_minority)), y_majority_downsampled))\n",
    "\n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312558b-f10f-4245-beb4-a4ce068208c7",
   "metadata": {},
   "source": [
    "# GET RANKED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8807e8-cca8-4fc5-bed9-e0b986e6256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "    'MLP': MLPClassifier(max_iter=500, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "selectors = {\n",
    "    'mRMR': lambda k: 'mRMR',\n",
    "    'SelectKBest_f': lambda k: SelectKBest(score_func=f_classif, k=k),\n",
    "    'SelectKBest_MI': lambda k: SelectKBest(score_func=mutual_info_classif, k=k),\n",
    "    'GenericUnivariateSelect': lambda k: GenericUnivariateSelect(score_func=f_classif, mode='k_best', param=k),\n",
    "    'VarianceThreshold': lambda k: VarianceThreshold(threshold=0.01),\n",
    "    'ModelBased_XGB': lambda k: 'ModelBased_XGB',\n",
    "    'ModelBased_LGBM': lambda k: 'ModelBased_LGBM',\n",
    "    'ModelBased_RF': lambda k: 'ModelBased_RF',\n",
    "    'ModelBased_CatBoost': lambda k: 'ModelBased_CatBoost',\n",
    "    'ModelBased_ExtraTrees': lambda k: 'ModelBased_ExtraTrees',\n",
    "}\n",
    "\n",
    "# Feature ranking function\n",
    "def get_ranked_features(X, y, method):\n",
    "    if method == \"SelectKBest_f\":\n",
    "        selector = SelectKBest(score_func=f_classif, k='all')\n",
    "        selector.fit(X, y)\n",
    "        scores = selector.scores_\n",
    "        return list(X.columns[np.argsort(scores)[::-1]])\n",
    "    elif method == \"SelectKBest_MI\":\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "        selector.fit(X, y)\n",
    "        scores = selector.scores_\n",
    "        return list(X.columns[np.argsort(scores)[::-1]])\n",
    "    elif method == \"GenericUnivariateSelect\":\n",
    "        selector = GenericUnivariateSelect(score_func=f_classif, mode='k_best', param='all')\n",
    "        selector.fit(X, y)\n",
    "        scores = selector.scores_\n",
    "        return list(X.columns[np.argsort(scores)[::-1]])\n",
    "    elif method == \"VarianceThreshold\":\n",
    "        variances = X.var()\n",
    "        return list(X.columns[np.argsort(variances.values)[::-1]])\n",
    "    elif method == \"ModelBased_XGB\":\n",
    "        model = XGBClassifier(eval_metric='logloss', random_state=42, verbosity=0)\n",
    "        model.fit(X, y)\n",
    "        importances = model.feature_importances_\n",
    "        return list(X.columns[np.argsort(importances)[::-1]])\n",
    "    elif method == \"ModelBased_LGBM\":\n",
    "        model = LGBMClassifier(random_state=42, verbose=-1)\n",
    "        model.fit(X, y)\n",
    "        importances = model.booster_.feature_importance(importance_type='gain')\n",
    "        return list(X.columns[np.argsort(importances)[::-1]])\n",
    "    elif method == \"ModelBased_RF\":\n",
    "        model = RandomForestClassifier(random_state=42, verbose=0)\n",
    "        model.fit(X, y)\n",
    "        importances = model.feature_importances_\n",
    "        return list(X.columns[np.argsort(importances)[::-1]])\n",
    "    elif method == \"ModelBased_CatBoost\":\n",
    "        model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "        model.fit(X, y)\n",
    "        importances = model.get_feature_importance()\n",
    "        return list(X.columns[np.argsort(importances)[::-1]])\n",
    "    elif method == \"ModelBased_ExtraTrees\":\n",
    "        model = ExtraTreesClassifier(random_state=42, verbose=0)\n",
    "        model.fit(X, y)\n",
    "        importances = model.feature_importances_\n",
    "        return list(X.columns[np.argsort(importances)[::-1]])\n",
    "    elif method == \"mRMR\":\n",
    "        return mrmr_classif(X, y, K=X.shape[1])\n",
    "    else:\n",
    "        raise ValueError(f\"Método de selección desconocido: {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72655710-d64c-48f1-b8a4-004414a23c26",
   "metadata": {},
   "source": [
    "# EXTRACT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755aff87-e15f-4f84-8106-1cbadee6920e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:05<00:00, 13.27it/s]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.35it/s]\n",
      "100%|██████████| 64/64 [00:04<00:00, 13.06it/s]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.79it/s]\n"
     ]
    }
   ],
   "source": [
    "ranked_features = {}\n",
    "\n",
    "# ========== LOAD AND DATA PROCESSING ==========\n",
    "df = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "no_significativas = [\n",
    "    \n",
    "    # variables with more than 30% missings\n",
    "    \"pct_median\", \"pct_min\", \"pct_max\",\n",
    "\n",
    "    # non-significant categorical variables in 24h prior the event\n",
    "    \"AdmType_scheduled\", \"AdmType_urgent\",\n",
    "    \"MPOC\",\n",
    "    \"dias_VMI\",\n",
    "    \"ICU_days\",\n",
    "    \"exitus\",\n",
    "    \"consistencia_secreciones_altres\", \"consistencia_secreciones_espesses\", \"consistencia_secreciones_fluides\", \"consistencia_secreciones_taps_de_moc\",\n",
    "    \"reintubacion\"\n",
    "]\n",
    "df.drop(columns=no_significativas, inplace=True)\n",
    "\n",
    "res = run_by_window(df, n_probes = 10)\n",
    "lista_features = res['kept_features']\n",
    "\n",
    "df = df[['PatientID', 'hr', 'NAV'] + lista_features].copy()\n",
    "\n",
    "for hr in [-1, -24 , -48]:\n",
    "\n",
    "    df_hr = df[df['hr'] == hr].drop(columns=['PatientID', 'hr'], axis=1).reset_index(drop=True)\n",
    "    X = df_hr.drop(columns=['NAV'], axis=1)\n",
    "    y = df_hr['NAV']\n",
    "    columns = list(X.columns)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X.columns)\n",
    "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X.columns)\n",
    "    \n",
    "    X_train, y_train = downsampling(X_train.values, y_train, majority_proportion=1.0)\n",
    "    X_train = pd.DataFrame(X_train, columns=columns)\n",
    "    \n",
    "    hr_str = f\"hr={hr}\"\n",
    "    ranked_features[hr_str] = {}\n",
    "    for selector_name, scores in selectors.items():\n",
    "        ranked_features[hr_str][selector_name] = get_ranked_features(X_train, y_train, selector_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd465a-af22-4474-b670-38f919ce1b96",
   "metadata": {},
   "source": [
    "# MODEL HYPERPARAMETER TUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a4097-bf7a-4bb4-8403-94f9bd82f2ac",
   "metadata": {},
   "source": [
    "https://forecastegy.com/posts/catboost-hyperparameter-tuning-guide-with-optuna/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638f8c6-1325-4d17-af09-137f61750cf9",
   "metadata": {},
   "source": [
    "## MODEL -1H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea4d054-3072-4771-a3b3-4958096237c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 32\n",
    "best_feature_selector = 'ModelBased_CatBoost'\n",
    "\n",
    "features_finales_1h = ranked_features['hr=-1'][best_feature_selector][:n_features]\n",
    "features_finales_1h_nav = features_finales_1h.copy()\n",
    "features_finales_1h_nav.extend(['NAV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c415828b-e442-4fd8-8645-bdab0901c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_training = df[df.hr==-1].reset_index(drop=True)\n",
    "\n",
    "lista_pacientes = df_training.PatientID.unique()\n",
    "df_test_24 = df[(df.PatientID.isin(lista_pacientes)) & (df.hr==-24)].reset_index(drop=True)\n",
    "\n",
    "df_training = df_training[features_finales_1h_nav]\n",
    "df_test_24 = df_test_24[features_finales_1h_nav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd5e0d6-904e-4347-9954-56679ef7d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_training.drop(\"NAV\", axis=1).copy()\n",
    "y = df_training[\"NAV\"].copy()\n",
    "X_24 = df_test_24.drop(\"NAV\", axis=1).copy()\n",
    "y_24 = df_test_24[\"NAV\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_24, X_test_24, y_train_24, y_test_24 = train_test_split(X_24, y_24, test_size=0.2, random_state=42, stratify=y_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83314cac-8d3a-41cc-9d7d-1f20aa454cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Espacio de hiperparámetros para CatBoost\n",
    "    param_space = {\n",
    "        'depth': trial.suggest_int('depth', 3, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 500, step=50),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0, step=0.1)\n",
    "    }\n",
    "\n",
    "    # Hiperparámetros adicionales\n",
    "    majority_proportion = trial.suggest_categorical('majority_proportion', [0.8, 0.9, 1.0, 1.1, 1.2])\n",
    "    n_folds = trial.suggest_categorical('n_folds', [3, 4, 5])\n",
    "\n",
    "    # Validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    pr_aucs = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_idx].copy(), X_train.iloc[valid_idx].copy()\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_idx].copy(), y_train.iloc[valid_idx].copy()\n",
    "        X_train_fold_24, X_valid_fold_24 = X_train_24.iloc[train_idx].copy(), X_train_24.iloc[valid_idx].copy()\n",
    "        y_train_fold_24, y_valid_fold_24 = y_train_24.iloc[train_idx].copy(), y_train_24.iloc[valid_idx].copy()\n",
    "\n",
    "        # Imputación sin leakage\n",
    "        imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "        X_train_fold = imputer.fit_transform(X_train_fold)\n",
    "        # X_valid_fold = imputer.transform(X_valid_fold)\n",
    "        X_valid_fold_24 = imputer.transform(X_valid_fold_24)\n",
    "\n",
    "        # Downsampling dentro del fold\n",
    "        X_train_fold, y_train_fold = downsampling(X_train_fold, y_train_fold, majority_proportion=majority_proportion)\n",
    "\n",
    "        # Entrenamiento y evaluación\n",
    "        model = CatBoostClassifier(\n",
    "            **param_space,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_proba = model.predict_proba(X_valid_fold_24)[:, 1]\n",
    "\n",
    "        # PR-AUC\n",
    "        pr_auc = average_precision_score(y_valid_fold_24, y_pred_proba)\n",
    "        pr_aucs.append(pr_auc)\n",
    "\n",
    "    return np.mean(pr_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7444273-dcd5-42b2-b6de-3b682e733145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3886d98c-81d5-4963-af70-4b4b347e2a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority proportion: 1.2\n",
      "N-Folds: 4\n",
      "Parameters: {'depth': 6, 'learning_rate': 0.05771714192617806, 'iterations': 450, 'l2_leaf_reg': 1.4446122553510594, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "# Puedes agregar los que estaban fuera de param_space si los usaste\n",
    "best_majority_proportion = best_params.pop(\"majority_proportion\")\n",
    "best_n_folds = best_params.pop(\"n_folds\")\n",
    "\n",
    "print(f\"Majority proportion: {best_majority_proportion}\")\n",
    "print(f\"N-Folds: {best_n_folds}\")\n",
    "print(f\"Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e63f6c00-8f30-4cca-a2aa-79ec3af06819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f98c69f7190>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_train_total = df_train.copy()\n",
    "df_train = df_train[df_train.hr==-1].reset_index(drop=True)\n",
    "df_train = df_train[features_finales_1h_nav]\n",
    "\n",
    "X = df_train.drop(\"NAV\", axis=1).copy()\n",
    "y = df_train[\"NAV\"].copy()\n",
    "X_total = df_train_total[\"PatientID\"].copy()\n",
    "y_total = df_train_total[\"NAV\"].copy()\n",
    "columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "_, X_test_patients, _, _ = train_test_split(X_total, y_total, test_size=0.2, random_state=42, stratify=y_total)\n",
    "X_test_patients = X_test_patients.values\n",
    "\n",
    "# Imputación y downsampling final\n",
    "imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "X_test_imp_df = pd.DataFrame(X_test_imp, columns=columns)\n",
    "\n",
    "X_train_ds, y_train_ds = downsampling(X_train_imp, y_train, majority_proportion=best_majority_proportion)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "final_model = CatBoostClassifier(\n",
    "    **best_params,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_ds, y_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "910429fd-51a7-45dc-ace4-62bcd3798ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save_model(\"models/best_model_1h.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cd5edb4-cd0c-4990-b5ea-ce90b8297d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f99248b4d50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = CatBoostClassifier()\n",
    "final_model.load_model(\"models/best_model_1h.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7d79e2d-a939-4636-9e14-187f053e5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_test_total = df_test.copy()\n",
    "df_test = df_test[df_test.hr==-24].reset_index(drop=True)\n",
    "df_test = df_test[features_finales_1h_nav]\n",
    "\n",
    "X = df_test.drop(\"NAV\", axis=1).copy()\n",
    "y = df_test[\"NAV\"].copy()\n",
    "X_total = df_test_total[\"PatientID\"].copy()\n",
    "y_total = df_test_total[\"NAV\"].copy()\n",
    "columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "_, X_test_patients, _, _ = train_test_split(X_total, y_total, test_size=0.2, random_state=42, stratify=y_total)\n",
    "X_test_patients = X_test_patients.values\n",
    "\n",
    "# Imputación y downsampling final\n",
    "imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "X_test_imp_df = pd.DataFrame(X_test_imp, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe6558ce-1825-4204-94f4-24c3a62f345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = final_model.predict_proba(X_test_imp)[:, 1]\n",
    "y_pred = (y_pred_prob > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23df0529-aabe-4505-8c76-7a3162891cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tY PRED\n",
      "Accuracy: 0.81\n",
      "Recall: 0.72\n",
      "Confusion matrix:\n",
      " [[ 48  19]\n",
      " [ 87 410]]\n",
      "AUC: 0.83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, recall_score,\n",
    "    precision_score, f1_score, confusion_matrix,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = np.flipud(cm)\n",
    "cm = np.fliplr(cm)\n",
    "fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred_prob)\n",
    "auc_ann = auc(fpr_ann, tpr_ann)\n",
    "\n",
    "print(f\"\\tY PRED\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Confusion matrix:\\n\",cm)\n",
    "print(f\"AUC: {auc_ann:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1767a67-6d0a-4635-9af8-6fe7e6a356cf",
   "metadata": {},
   "source": [
    "## MODEL -24H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc09d5e6-336c-4321-9827-443d0fca5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 36\n",
    "best_feature_selector = 'mRMR'\n",
    "\n",
    "features_finales_24h = ranked_features['hr=-24'][best_feature_selector][:n_features]\n",
    "features_finales_24h_nav = features_finales_24h.copy()\n",
    "features_finales_24h_nav.extend(['NAV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76429d4-c642-40b3-9811-8ed3c611a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_training = df_training[df_training.hr==-24].reset_index(drop=True)\n",
    "df_training = df_training[features_finales_24h_nav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5082af94-b21c-45d0-9e7f-e1aeb530bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_training.drop(\"NAV\", axis=1).copy()\n",
    "y = df_training[\"NAV\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a95a0d-dd6d-4c63-bb90-25642b48a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Espacio de hiperparámetros para CatBoost\n",
    "    param_space = {\n",
    "        'depth': trial.suggest_int('depth', 3, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 500, step=50),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0, step=0.1)\n",
    "    }\n",
    "\n",
    "    # Hiperparámetros adicionales\n",
    "    majority_proportion = trial.suggest_categorical('majority_proportion', [0.8, 0.9, 1.0, 1.1, 1.2])\n",
    "    n_folds = trial.suggest_categorical('n_folds', [3, 4, 5])\n",
    "\n",
    "    # Validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    pr_aucs = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_idx].copy(), X_train.iloc[valid_idx].copy()\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_idx].copy(), y_train.iloc[valid_idx].copy()\n",
    "\n",
    "        # Imputación sin leakage\n",
    "        imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "        X_train_fold = imputer.fit_transform(X_train_fold)\n",
    "        X_valid_fold = imputer.transform(X_valid_fold)\n",
    "\n",
    "        # Downsampling dentro del fold\n",
    "        X_train_fold, y_train_fold = downsampling(X_train_fold, y_train_fold, majority_proportion=majority_proportion)\n",
    "\n",
    "        # Entrenamiento y evaluación\n",
    "        model = CatBoostClassifier(\n",
    "            **param_space,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_proba = model.predict_proba(X_valid_fold)[:, 1]\n",
    "\n",
    "        # PR-AUC\n",
    "        pr_auc = average_precision_score(y_valid_fold, y_pred_proba)\n",
    "        pr_aucs.append(pr_auc)\n",
    "\n",
    "    return np.mean(pr_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161490f-8799-41b6-b6a9-32c7b7c1b9c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ea3bc-4389-470b-9637-5e4f02c44db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "plot_parallel_coordinate(study)\n",
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d73d01-e60b-42c7-a59d-4617fcc18708",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "# Puedes agregar los que estaban fuera de param_space si los usaste\n",
    "best_majority_proportion = best_params.pop(\"majority_proportion\")\n",
    "best_n_folds = best_params.pop(\"n_folds\")\n",
    "\n",
    "print(f\"Majority proportion: {best_majority_proportion}\")\n",
    "print(f\"N-Folds: {best_n_folds}\")\n",
    "print(f\"Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9ca10d-75bd-4242-b186-137880a2f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'depth': 5, \n",
    "    'learning_rate': 0.017329985697213668, \n",
    "    'iterations': 500, \n",
    "    'l2_leaf_reg': 6.732343959458352, \n",
    "    'subsample': 0.7999999999999999\n",
    "}\n",
    "\n",
    "best_majority_proportion = 1.2\n",
    "best_n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58e081f-55c6-4b6b-bd94-59c9472f1e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f11450102d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_train_total = df_train.copy()\n",
    "df_train = df_train[df_train.hr==-24].reset_index(drop=True)\n",
    "df_train = df_train[features_finales_24h_nav]\n",
    "\n",
    "X = df_train.drop(\"NAV\", axis=1).copy()\n",
    "y = df_train[\"NAV\"].copy()\n",
    "X_total = df_train_total[\"PatientID\"].copy()\n",
    "y_total = df_train_total[\"NAV\"].copy()\n",
    "columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "_, X_test_patients, _, _ = train_test_split(X_total, y_total, test_size=0.2, random_state=42, stratify=y_total)\n",
    "X_test_patients = X_test_patients.values\n",
    "\n",
    "# Imputación y downsampling final\n",
    "imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "X_test_imp_df = pd.DataFrame(X_test_imp, columns=columns)\n",
    "\n",
    "X_train_ds, y_train_ds = downsampling(X_train_imp, y_train, majority_proportion=best_majority_proportion)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "final_model = CatBoostClassifier(\n",
    "    **best_params,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_ds, y_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff792f71-3365-4692-84a5-ab94dc57fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save_model(\"models/best_model_24h.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7140739e-c5bc-4678-b047-1ea99f9d36ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f11b0d5ba10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = CatBoostClassifier()\n",
    "final_model.load_model(\"models/best_model_24h.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9c27e6-3727-4405-83af-2c251a80a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_test_total = df_test.copy()\n",
    "df_test = df_test[df_test.hr==-24].reset_index(drop=True)\n",
    "df_test = df_test[features_finales_24h_nav]\n",
    "\n",
    "X = df_test.drop(\"NAV\", axis=1).copy()\n",
    "y = df_test[\"NAV\"].copy()\n",
    "X_total = df_test_total[\"PatientID\"].copy()\n",
    "y_total = df_test_total[\"NAV\"].copy()\n",
    "columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "_, X_test_patients, _, _ = train_test_split(X_total, y_total, test_size=0.2, random_state=42, stratify=y_total)\n",
    "X_test_patients = X_test_patients.values\n",
    "\n",
    "# Imputación y downsampling final\n",
    "imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "X_test_imp_df = pd.DataFrame(X_test_imp, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8048e75-3657-4e0e-b4dc-f13bb366b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = final_model.predict_proba(X_test_imp)[:, 1]\n",
    "y_pred = (y_pred_prob > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c3b775-4b12-4e55-9abb-bc3e07418b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tY PRED\n",
      "Accuracy: 0.83\n",
      "Recall: 0.76\n",
      "Confusion matrix:\n",
      " [[ 51  16]\n",
      " [ 81 416]]\n",
      "AUC: 0.86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, recall_score,\n",
    "    precision_score, f1_score, confusion_matrix,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = np.flipud(cm)\n",
    "cm = np.fliplr(cm)\n",
    "fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred_prob)\n",
    "auc_ann = auc(fpr_ann, tpr_ann)\n",
    "\n",
    "print(f\"\\tY PRED\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Confusion matrix:\\n\",cm)\n",
    "print(f\"AUC: {auc_ann:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282154ed-460d-4fc1-9b2e-7b49f37ed26c",
   "metadata": {},
   "source": [
    "## MODEL -48H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed8a1df8-29c7-4e3e-bd6d-796ab416a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 25\n",
    "best_feature_selector = 'ModelBased_CatBoost'\n",
    "\n",
    "features_finales_48h = ranked_features['hr=-48'][best_feature_selector][:n_features]\n",
    "features_finales_48h_nav = features_finales_48h.copy()\n",
    "features_finales_48h_nav.extend(['NAV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "267f85e7-89fb-4a6b-b4a4-9b0e54006bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_training = df[df.hr==-48].reset_index(drop=True)\n",
    "\n",
    "lista_pacientes = df_training.PatientID.unique()\n",
    "df_test_24 = df[(df.PatientID.isin(lista_pacientes)) & (df.hr==-24)].reset_index(drop=True)\n",
    "\n",
    "df_training = df_training[features_finales_48h_nav]\n",
    "df_test_24 = df_test_24[features_finales_48h_nav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d809fac-fdbe-4a00-a5fb-24789e22ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_training.drop(\"NAV\", axis=1).copy()\n",
    "y = df_training[\"NAV\"].copy()\n",
    "X_24 = df_test_24.drop(\"NAV\", axis=1).copy()\n",
    "y_24 = df_test_24[\"NAV\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_24, X_test_24, y_train_24, y_test_24 = train_test_split(X_24, y_24, test_size=0.2, random_state=42, stratify=y_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4e0e304-0323-4379-ab81-f949436d1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Espacio de hiperparámetros para CatBoost\n",
    "    param_space = {\n",
    "        'depth': trial.suggest_int('depth', 3, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 500, step=50),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0, step=0.1)\n",
    "    }\n",
    "\n",
    "    # Hiperparámetros adicionales\n",
    "    majority_proportion = trial.suggest_categorical('majority_proportion', [0.8, 0.9, 1.0, 1.1, 1.2])\n",
    "    n_folds = trial.suggest_categorical('n_folds', [3, 4, 5])\n",
    "\n",
    "    # Validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    pr_aucs = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_idx].copy(), X_train.iloc[valid_idx].copy()\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_idx].copy(), y_train.iloc[valid_idx].copy()\n",
    "        X_train_fold_24, X_valid_fold_24 = X_train_24.iloc[train_idx].copy(), X_train_24.iloc[valid_idx].copy()\n",
    "        y_train_fold_24, y_valid_fold_24 = y_train_24.iloc[train_idx].copy(), y_train_24.iloc[valid_idx].copy()\n",
    "\n",
    "        # Imputación sin leakage\n",
    "        imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "        X_train_fold = imputer.fit_transform(X_train_fold)\n",
    "        # X_valid_fold = imputer.transform(X_valid_fold)\n",
    "        X_valid_fold_24 = imputer.transform(X_valid_fold_24)\n",
    "\n",
    "        # Downsampling dentro del fold\n",
    "        X_train_fold, y_train_fold = downsampling(X_train_fold, y_train_fold, majority_proportion=majority_proportion)\n",
    "\n",
    "        # Entrenamiento y evaluación\n",
    "        model = CatBoostClassifier(\n",
    "            **param_space,\n",
    "            verbose=0,\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_proba = model.predict_proba(X_valid_fold_24)[:, 1]\n",
    "\n",
    "        # PR-AUC\n",
    "        pr_auc = average_precision_score(y_valid_fold_24, y_pred_proba)\n",
    "        pr_aucs.append(pr_auc)\n",
    "\n",
    "    return np.mean(pr_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b735835-cc9e-41d7-9a6f-d77629a61287",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e187ad31-5fab-4041-9432-c6b908752e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority proportion: 1.2\n",
      "N-Folds: 3\n",
      "Parameters: {'depth': 4, 'learning_rate': 0.016918242732133534, 'iterations': 200, 'l2_leaf_reg': 8.534044277665194, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "# Puedes agregar los que estaban fuera de param_space si los usaste\n",
    "best_majority_proportion = best_params.pop(\"majority_proportion\")\n",
    "best_n_folds = best_params.pop(\"n_folds\")\n",
    "\n",
    "print(f\"Majority proportion: {best_majority_proportion}\")\n",
    "print(f\"N-Folds: {best_n_folds}\")\n",
    "print(f\"Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7099ae9d-a39c-4c06-9da8-9ce72a4f1d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f98c695e950>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_train_total = df_train.copy()\n",
    "df_train = df_train[df_train.hr==-48].reset_index(drop=True)\n",
    "df_train = df_train[features_finales_48h_nav]\n",
    "\n",
    "X = df_train.drop(\"NAV\", axis=1).copy()\n",
    "y = df_train[\"NAV\"].copy()\n",
    "X_total = df_train_total[\"PatientID\"].copy()\n",
    "y_total = df_train_total[\"NAV\"].copy()\n",
    "columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "_, X_test_patients, _, _ = train_test_split(X_total, y_total, test_size=0.2, random_state=42, stratify=y_total)\n",
    "X_test_patients = X_test_patients.values\n",
    "\n",
    "# Imputación y downsampling final\n",
    "imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "X_test_imp_df = pd.DataFrame(X_test_imp, columns=columns)\n",
    "\n",
    "X_train_ds, y_train_ds = downsampling(X_train_imp, y_train, majority_proportion=best_majority_proportion)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "final_model = CatBoostClassifier(\n",
    "    **best_params,\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_ds, y_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10bdfe5b-07f6-4e16-a0e9-d463941465aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save_model(\"models/best_model_48h.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16fcb80d-fc27-4d45-bc78-caf9b5c9cd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f98b5a4a790>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = CatBoostClassifier()\n",
    "final_model.load_model(\"models/best_model_48h.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9c204af9-20f4-4a01-8765-ac1dfaf827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/extraction/nav_processed_v2.pkl\")\n",
    "df_test_total = df.copy()\n",
    "df_train = df[df.hr==-48].reset_index(drop=True)\n",
    "pacientes_48h = df_train.PatientID.unique()\n",
    "df_test = df[(df.hr==-24) & (df.PatientID.isin(pacientes_48h))].reset_index(drop=True)\n",
    "df_train = df_train[features_finales_48h_nav]\n",
    "df_test = df_test[features_finales_48h_nav]\n",
    "\n",
    "X = df_train.drop(\"NAV\", axis=1).copy()\n",
    "y = df_train[\"NAV\"].copy()\n",
    "X_24 = df_test.drop(\"NAV\", axis=1).copy()\n",
    "y_24 = df_test[\"NAV\"].copy()\n",
    "X_total = df_test_total[\"PatientID\"].copy()\n",
    "y_total = df_test_total[\"NAV\"].copy()\n",
    "columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_24, X_test_24, y_train_24, y_test_24 = train_test_split(X_24, y_24, test_size=0.2, random_state=42, stratify=y_24)\n",
    "_, X_test_patients, _, _ = train_test_split(X_total, y_total, test_size=0.2, random_state=42, stratify=y_total)\n",
    "X_test_patients = X_test_patients.values\n",
    "\n",
    "# Imputación y downsampling final\n",
    "imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test_24)\n",
    "X_test_imp_df = pd.DataFrame(X_test_imp, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f461b523-5b71-4d1b-8ee2-def4d01e3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = final_model.predict_proba(X_test_imp)[:, 1]\n",
    "y_pred = (y_pred_prob > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0ec1c348-4e45-4be0-8e09-2af844781c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tY PRED\n",
      "Accuracy: 0.75\n",
      "Recall: 0.61\n",
      "Confusion matrix:\n",
      " [[ 39  25]\n",
      " [102 347]]\n",
      "AUC: 0.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, recall_score,\n",
    "    precision_score, f1_score, confusion_matrix,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = np.flipud(cm)\n",
    "cm = np.fliplr(cm)\n",
    "fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred_prob)\n",
    "auc_ann = auc(fpr_ann, tpr_ann)\n",
    "\n",
    "print(f\"\\tY PRED\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Confusion matrix:\\n\",cm)\n",
    "print(f\"AUC: {auc_ann:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
